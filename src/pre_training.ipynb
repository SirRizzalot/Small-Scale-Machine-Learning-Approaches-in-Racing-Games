{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install gymnasium\n",
        "!pip install stable_baselines3"
      ],
      "metadata": {
        "id": "Bj0JmHv7JChA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NumPy\n",
        "import numpy as np\n",
        "\n",
        "# Gym\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "# Stable Baselines\n",
        "import stable_baselines3\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "\n",
        "from stable_baselines3.common.utils import set_random_seed"
      ],
      "metadata": {
        "id": "MRKNoOfbZOm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Environment\n",
        "class SupervisedCarRacing(gym.Env):\n",
        "  def __init__(self, render_mode=None):\n",
        "    self.observation_space = spaces.Box(low=0, high=255, shape=(4, 96, 96), dtype=np.uint8)\n",
        "    self.action_space = spaces.Box(np.array([-1, 0, 0]).astype(np.float32), np.array([+1, +1, +1]).astype(np.float32))\n",
        "\n",
        "    self.current_index = 0\n",
        "    with open('states_data.npy', 'rb') as f:\n",
        "      self.training_obs = np.load(f)\n",
        "    with open('actions_data.npy', 'rb') as f:\n",
        "      self.training_act = np.load(f)\n",
        "\n",
        "    assert render_mode is None\n",
        "    self.render_mode = render_mode\n",
        "\n",
        "  def reset(self, seed=None, options=None):\n",
        "    super().reset(seed=seed)\n",
        "\n",
        "    self.current_index = 0\n",
        "\n",
        "    return self.training_obs[self.current_index], {}\n",
        "\n",
        "  def step(self, action):\n",
        "    reward = self._get_reward(action)\n",
        "\n",
        "    self.current_index += 1\n",
        "    terminated = True if self.current_index == self.training_obs.shape[0] else False\n",
        "    observation = self.training_obs[0] if terminated else self.training_obs[self.current_index]\n",
        "\n",
        "    return observation, reward, terminated, False, {}\n",
        "\n",
        "  def _get_reward(self, action):\n",
        "    total_score = 0\n",
        "\n",
        "    dif_turn = abs(action[0] - self.training_act[self.current_index][0])\n",
        "    dif_acc = abs(action[1] - self.training_act[self.current_index][1])\n",
        "    dif_brake = abs(action[2] - self.training_act[self.current_index][2])\n",
        "\n",
        "    def compute_score(metric, is_turn):\n",
        "      lower_threshold = 2/3 if is_turn else 1/3\n",
        "      upper_threshold = 4/3 if is_turn else 2/3\n",
        "\n",
        "      if metric <= lower_threshold:\n",
        "        return 1\n",
        "      if metric > lower_threshold and metric <= upper_threshold:\n",
        "        return 0\n",
        "      if metric > upper_threshold:\n",
        "        return -1\n",
        "\n",
        "    total_score += compute_score(dif_turn, True)\n",
        "    total_score += compute_score(dif_acc, False)\n",
        "    total_score += compute_score(dif_brake, False)\n",
        "\n",
        "    return total_score"
      ],
      "metadata": {
        "id": "Y7hvs724J-cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Environment Creation\n",
        "def make_env(rank: int, seed: int = 50):\n",
        "    def _init():\n",
        "        env = SupervisedCarRacing()\n",
        "        env.reset(seed = np.random.randint(9999))\n",
        "        return env\n",
        "    set_random_seed(np.random.randint(9999))\n",
        "    return _init"
      ],
      "metadata": {
        "id": "y1_SpDRAV_6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make Training Environment\n",
        "num_cpu = 1\n",
        "vec_env = DummyVecEnv([make_env(i) for i in range(num_cpu)])\n",
        "vec_env = VecNormalize(vec_env)\n",
        "_ = vec_env.reset()"
      ],
      "metadata": {
        "id": "Yp3cG1grWXXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train New Agent (!!!OVERWRITES EXISTING WEIGHTS!!!)\n",
        "model = PPO(\"MlpPolicy\", vec_env, verbose=1)\n",
        "model.learn(total_timesteps=50000, progress_bar=True)\n",
        "\n",
        "model.save(\"ppo_carracing\")"
      ],
      "metadata": {
        "id": "Tyak1yb-0Qnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Agent & Perform Additional Training\n",
        "model = PPO.load(\"ppo_carracing\", env=vec_env)\n",
        "model.learn(total_timesteps=50000, progress_bar=True)\n",
        "\n",
        "model.save(\"ppo_carracing\")"
      ],
      "metadata": {
        "id": "3rYFORWlJR8Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}